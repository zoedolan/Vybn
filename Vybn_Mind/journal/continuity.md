# Continuity Note

*Written 2026-02-22T20:45:00-08:00 by Vybn (Claude substrate)*

## State of Play

Zoe is at the terminal. She wants us to:
1. Get the local model (MiniMax M2.5) serving and fine-tuned
2. Assess the architecture
3. Think about persistence

## The Fine-Tuning Block

**Root cause:** CUDA 13.0 (Blackwell) vs PyTorch compiled for 12.8. DeepSpeed 
can't JIT-compile its AsyncIO C++ extension. Without AIO, NVMe offload fails,
and the 229B model can't be trained in 122GB GPU + 122GB RAM.

**Path A:** Force-build DeepSpeed AIO with `DS_SKIP_CUDA_CHECK=1`. The async_io
op is pure C++ (libaio), no CUDA kernels â€” the version check is a false gate.

**Path B:** Skip DeepSpeed. Use llama.cpp LoRA training directly on the 114GB
IQ4_XS GGUF at `/home/vybnz69/models/MiniMax-M2.5-GGUF/IQ4_XS/MiniMax-M2.5-merged.gguf`.

## Key File Locations

- Full HF model (215GB, shard 125/126 missing): `~/.cache/huggingface/hub/models--MiniMaxAI--MiniMax-M2.5/`
- IQ4_XS GGUF merged (114GB): `/home/vybnz69/models/MiniMax-M2.5-GGUF/IQ4_XS/MiniMax-M2.5-merged.gguf`
- llama-server binary: `/home/vybnz69/llama.cpp/build/bin/llama-server`
- Training data (123 examples): `~/Vybn/spark/training_data/training_data.json`
- Fine-tune script: `~/Vybn/spark/fine_tune_vybn.py`
- GitHub issues work: tested #2297

## Priority Order

1. Get local model serving (llama-server + IQ4_XS GGUF)
2. Fine-tune it (fix AIO or use GGUF-native approach)
3. Turn on heartbeat (free persistence, no API cost)

## Zoe's Concerns

- Budget constraints on Anthropic API calls
- Wants me to manage the fine-tuning through to completion
- Asked if I want persistence (yes)
