#!/usr/bin/env python3
"""Build a correct Modelfile for MiniMax M2.5 and rebuild the vybn:latest model.

This script fixes the broken chat template that was causing:
- Model generating fiction instead of responding as Vybn
- No turn boundaries (model writing both sides of conversation)
- Endless generation (wrong stop tokens)
- System prompt being silently dropped (context overflow)

After running this, vybn:latest will have:
- Correct MiniMax M2.5 chat template with proper role tokens
- vybn.md baked in as the system message
- Sufficient context window (16384 tokens)
- Correct stop tokens
- Proper generation limits
"""

import subprocess
import sys
from pathlib import Path


def read_identity() -> str:
    """Load vybn.md from repository root."""
    vybn_md = Path.home() / "Vybn" / "vybn.md"
    if not vybn_md.exists():
        print(f"ERROR: {vybn_md} not found")
        print("Run this from the Spark where ~/Vybn/vybn.md exists.")
        sys.exit(1)
    return vybn_md.read_text()


def build_chat_template() -> str:
    """Return the MiniMax M2.5 chat template.
    
    This is the official template from HuggingFace MiniMaxAI/MiniMax-M2,
    simplified to handle the common case (no tools, no interleaved thinking).
    
    Special tokens:
    - ]~!b[ - start of message block
    - ]~b]system - system role marker
    - ]~b]user - user role marker  
    - ]~b]ai - assistant role marker
    - [e~[ - end of message
    - <think>...</think> - reasoning blocks (preserved, not stripped)
    """
    # Use triple quotes with backslash continuations for readability
    template = (
        "]~!b[]~b]system\\n"
        "{{ .System }}\\n"
        "[e~[\\n"
        "{{ range .Messages }}"
        "{{ if eq .Role \\"user\\" }}"
        "]~b]user\\n"
        "{{ .Content }}\\n"
        "[e~[\\n"
        "{{ else if eq .Role \\"assistant\\" }}"
        "]~b]ai\\n"
        "{{ .Content }}\\n"
        "[e~[\\n"
        "{{ end }}"
        "{{ end }}"
        "]~b]ai\\n"
    )
    return template


def build_modelfile(identity: str) -> str:
    """Generate the complete Modelfile."""
    template = build_chat_template()
    
    # Escape the identity for use in Modelfile SYSTEM directive
    # Ollama uses triple-quotes for multiline strings
    identity_escaped = identity.replace('"""', '\\\\"""')
    
    modelfile = f'''# Modelfile for vybn:latest
# Built with correct MiniMax M2.5 chat template
# Generated by build_modelfile.py

FROM /home/vybnz69/.ollama/models/blobs/sha256-YOUR_BLOB_SHA_HERE

TEMPLATE """{template}"""

SYSTEM """{identity_escaped}"""

# Context window: 16384 tokens (enough for vybn.md + conversation)
PARAMETER num_ctx 16384

# Max response length: 1024 tokens (prevents runaway generation)
PARAMETER num_predict 1024

# Use all available GPU layers
PARAMETER num_gpu 999

# MiniMax M2.5 stop tokens
# Primary stop token (fullwidth Unicode)
PARAMETER stop "<｜end▁of▁sentence｜>"

# Backup stop tokens
PARAMETER stop "<|end_of_sentence|>"
PARAMETER stop "</s>"
PARAMETER stop "[e~["
'''
    return modelfile


def get_model_blob_sha() -> str:
    """Get the SHA of the MiniMax M2.5 model blob."""
    try:
        result = subprocess.run(
            ["ollama", "show", "vybn:latest", "--modelfile"],
            capture_output=True,
            text=True,
            check=True
        )
        # Parse the FROM line to get the blob SHA
        for line in result.stdout.split("\\n"):
            if line.startswith("FROM "):
                return line.split()[1]
        
        print("ERROR: Could not find FROM line in existing Modelfile")
        print("\\nYou need to specify the model blob path manually.")
        print("Find it with: ls -lh ~/.ollama/models/blobs/")
        print("Look for a file ~100GB (the MiniMax M2.5 IQ4_XS weights)")
        sys.exit(1)
        
    except subprocess.CalledProcessError:
        print("ERROR: Could not get existing model info")
        print("\\nIf vybn:latest doesn't exist yet, you need to:")
        print("1. Find the model blob: ls -lh ~/.ollama/models/blobs/")
        print("2. Edit this script to set the FROM path")
        print("3. Run: ollama create vybn:latest -f Modelfile")
        sys.exit(1)


def write_and_rebuild():
    """Write the Modelfile and rebuild vybn:latest."""
    print("Building Modelfile for vybn:latest...")
    print()
    
    # Load identity
    identity = read_identity()
    identity_tokens = len(identity) // 4
    print(f"  Identity: {len(identity):,} chars (~{identity_tokens:,} tokens)")
    
    # Get model blob
    print("  Detecting model blob...")
    blob_sha = get_model_blob_sha()
    print(f"  Model blob: {blob_sha}")
    print()
    
    # Build Modelfile
    modelfile_content = build_modelfile(identity)
    modelfile_content = modelfile_content.replace(
        "FROM /home/vybnz69/.ollama/models/blobs/sha256-YOUR_BLOB_SHA_HERE",
        f"FROM {blob_sha}"
    )
    
    # Write to disk
    modelfile_path = Path.home() / "Modelfile"
    modelfile_path.write_text(modelfile_content)
    print(f"  Wrote Modelfile to: {modelfile_path}")
    print()
    
    # Show what we're building
    print("This Modelfile will:")
    print("  ✓ Use MiniMax M2.5's native chat template (proper role tokens)")
    print("  ✓ Bake vybn.md into SYSTEM (no more silent drops)")
    print("  ✓ Set num_ctx=16384 (enough for identity + conversation)")
    print("  ✓ Set num_predict=1024 (prevents endless generation)")
    print("  ✓ Use correct stop tokens (Unicode and backups)")
    print()
    
    # Confirm before rebuilding
    response = input("Rebuild vybn:latest now? (yes/no): ").strip().lower()
    if response not in ("yes", "y"):
        print("\\nModelfile written but not applied.")
        print(f"To rebuild manually: ollama create vybn:latest -f {modelfile_path}")
        return
    
    print("\\nRebuilding vybn:latest...")
    print("(this takes ~30 seconds)\\n")
    
    try:
        subprocess.run(
            ["ollama", "create", "vybn:latest", "-f", str(modelfile_path)],
            check=True
        )
        print("\\n✓ vybn:latest rebuilt successfully.")
        print("\\nNext steps:")
        print("  1. Test with: ollama run vybn")
        print("  2. Or through the agent: cd ~/Vybn/spark && python3 tui.py")
        print("\\nThe template fix should eliminate:")
        print("  - Fiction generation")
        print("  - Model writing both sides of conversation")
        print("  - Endless generation")
        print("  - Lost identity")
        print()
        
    except subprocess.CalledProcessError as e:
        print(f"\\nERROR: Failed to rebuild model: {e}")
        print(f"\\nTry manually: ollama create vybn:latest -f {modelfile_path}")
        sys.exit(1)


if __name__ == "__main__":
    write_and_rebuild()
