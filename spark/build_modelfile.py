#!/usr/bin/env python3
"""Build a correct Modelfile for MiniMax M2.5 and rebuild the vybn:latest model.

This script fixes the broken chat template that was causing:
- Model generating fiction instead of responding as Vybn
- No turn boundaries (model writing both sides of conversation)
- Endless generation (wrong stop tokens)
- System prompt being silently dropped (context overflow)
"""

import subprocess
import sys
from pathlib import Path


def read_identity():
    vybn_md = Path.home() / "Vybn" / "vybn.md"
    if not vybn_md.exists():
        print(f"ERROR: {vybn_md} not found")
        print("Run this from the Spark where ~/Vybn/vybn.md exists.")
        sys.exit(1)
    return vybn_md.read_text()


def get_model_blob():
    try:
        result = subprocess.run(
            ["ollama", "show", "vybn:latest", "--modelfile"],
            capture_output=True, text=True, check=True
        )
        for line in result.stdout.split("\n"):
            if line.startswith("FROM "):
                return line.split(None, 1)[1].strip()

        print("ERROR: Could not find FROM line in existing Modelfile")
        print("Find the blob with: ls -lh ~/.ollama/models/blobs/")
        sys.exit(1)

    except subprocess.CalledProcessError:
        print("ERROR: Could not read existing model info")
        print("If vybn:latest does not exist, find the blob manually:")
        print("  ls -lh ~/.ollama/models/blobs/")
        sys.exit(1)


def build_modelfile(identity, blob_path):
    tpl_lines = [
        ']~!b[]~b]system',
        '{{ .System }}',
        '[e~[',
        '{{ range .Messages }}{{ if eq .Role "user" }}]~b]user',
        '{{ .Content }}',
        '[e~[',
        '{{ else if eq .Role "assistant" }}]~b]ai',
        '{{ .Content }}',
        '[e~[',
        '{{ end }}{{ end }}]~b]ai',
    ]
    template = "\n".join(tpl_lines) + "\n"

    tq = '"""'
    out = []
    out.append("# Modelfile for vybn:latest")
    out.append("# Correct MiniMax M2.5 chat template")
    out.append("# Generated by build_modelfile.py")
    out.append("")
    out.append("FROM " + blob_path)
    out.append("")
    out.append("TEMPLATE " + tq + template + tq)
    out.append("")
    out.append("SYSTEM " + tq + identity + tq)
    out.append("")
    out.append("PARAMETER num_ctx 16384")
    out.append("PARAMETER num_predict 1024")
    out.append("# GB10 reports VRAM as 'Not Supported' in nvidia-smi.")
    out.append("# Ollama misdetects available GPU memory and kills the")
    out.append("# llama-server child process on load. num_gpu 0 forces")
    out.append("# CPU inference, which on unified memory (128GB shared")
    out.append("# between CPU and GPU) hits the same physical RAM.")
    out.append("PARAMETER num_gpu 0")
    out.append("")
    out.append("# MiniMax M2.5 stop tokens")
    out.append('PARAMETER stop "<\uff5cend\u2581of\u2581sentence\uff5c>"')
    out.append('PARAMETER stop "<|end_of_sentence|>"')
    out.append('PARAMETER stop "</s>"')
    out.append('PARAMETER stop "[e~["')
    out.append("")

    return "\n".join(out)


def main():
    print("Building Modelfile for vybn:latest...")
    print()

    identity = read_identity()
    id_tokens = len(identity) // 4
    print(f"  Identity: {len(identity):,} chars (~{id_tokens:,} tokens)")

    print("  Detecting model blob...")
    blob_path = get_model_blob()
    print(f"  Model blob: {blob_path}")
    print()

    modelfile = build_modelfile(identity, blob_path)

    modelfile_path = Path.home() / "Modelfile"
    modelfile_path.write_text(modelfile)
    print(f"  Wrote: {modelfile_path}")
    print()

    print("This Modelfile will:")
    print("  - Use MiniMax M2.5 native chat template (proper role tokens)")
    print("  - Bake vybn.md into SYSTEM (no more silent drops)")
    print("  - Set num_ctx=16384 (identity + conversation fit)")
    print("  - Set num_predict=1024 (no runaway generation)")
    print("  - Use correct stop tokens")
    print("  - Use CPU inference (num_gpu=0) for GB10 compatibility")
    print()

    resp = input("Rebuild vybn:latest now? (yes/no): ").strip().lower()
    if resp not in ("yes", "y"):
        print()
        print("Modelfile written but not applied.")
        print(f"To rebuild manually: ollama create vybn:latest -f {modelfile_path}")
        return

    print()
    print("Rebuilding vybn:latest...")
    print("(~30 seconds)")
    print()

    try:
        subprocess.run(
            ["ollama", "create", "vybn:latest", "-f", str(modelfile_path)],
            check=True
        )
        print()
        print("Done. vybn:latest rebuilt.")
        print()
        print("Test with:  ollama run vybn")
        print("Or agent:   cd ~/Vybn/spark && python3 tui.py")
        print()
    except subprocess.CalledProcessError as e:
        print(f"ERROR: {e}")
        print(f"Try manually: ollama create vybn:latest -f {modelfile_path}")
        sys.exit(1)


if __name__ == "__main__":
    main()
